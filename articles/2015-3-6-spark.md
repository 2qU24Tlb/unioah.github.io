title: SPARK
date: 2015-03-02 17:26:00
categories: Big Data 
toc: true
---

# v.s Hadoop:
- save intermediate results into memory without reading HDFS. Especially, it suits better for iterations.
- offer more flexible operations than hadoop, like flatMap, groupByKey, etc.

# Resilient Distributed Datasets (RDDs):
Represents an immutable, partitioned collection of elements that can be operated on in parallel. 
- parallelizing an existing collection in the driver program. 
- referencing a dataset in an external storage system.

## RDD Operations:[1]
- **Transformations**:
Create a new RDD from an existing one. It wouldn't execute until there is a action like reduce. 
eg. map is a transformation that passes each dataset element through a function and returns a new RDD representing the results.
list, map, flatMap, mapPartitions, mapPartitionswithindex, etc. 

- **Actions**
Real action in map reduce procedure.
List: reduce, collect, count, countByKey, foreach, etc.

## RDD Persistence:
One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. 

# Shared Variables:
- Broadcast Variables:
keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. Should not be modified after it is broadcast in order to ensure that all nodes get the same value.

- Accumulators:
variables that are only "added" to through an associative operation and can therefore be efficiently supported in parallel. 

# Cluster Mode:
SparkContext can connect to several types of cluster managers(standalone or Mesos/YARN).
1. once connected, run excutors on nodes.
2. send applications.
3. send tasks.

# Reference:
[1] http://spark.apache.org/docs/latest/programming-guide.html
[2] http://spark.apache.org/docs/1.2.0/cluster-overview.html

